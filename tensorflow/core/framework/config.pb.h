// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: tensorflow/core/framework/config.proto

#ifndef GOOGLE_PROTOBUF_INCLUDED_tensorflow_2fcore_2fframework_2fconfig_2eproto
#define GOOGLE_PROTOBUF_INCLUDED_tensorflow_2fcore_2fframework_2fconfig_2eproto

#include <limits>
#include <string>

#include <google/protobuf/port_def.inc>
#if PROTOBUF_VERSION < 3016000
#error This file was generated by a newer version of protoc which is
#error incompatible with your Protocol Buffer headers. Please update
#error your headers.
#endif
#if 3016000 < PROTOBUF_MIN_PROTOC_VERSION
#error This file was generated by an older version of protoc which is
#error incompatible with your Protocol Buffer headers. Please
#error regenerate this file with a newer version of protoc.
#endif

#include <google/protobuf/port_undef.inc>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/arena.h>
#include <google/protobuf/arenastring.h>
#include <google/protobuf/generated_message_table_driven.h>
#include <google/protobuf/generated_message_util.h>
#include <google/protobuf/metadata_lite.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/message.h>
#include <google/protobuf/repeated_field.h>  // IWYU pragma: export
#include <google/protobuf/extension_set.h>  // IWYU pragma: export
#include <google/protobuf/map.h>  // IWYU pragma: export
#include <google/protobuf/map_entry.h>
#include <google/protobuf/map_field_inl.h>
#include <google/protobuf/unknown_field_set.h>
// @@protoc_insertion_point(includes)
#include <google/protobuf/port_def.inc>
#define PROTOBUF_INTERNAL_EXPORT_tensorflow_2fcore_2fframework_2fconfig_2eproto
PROTOBUF_NAMESPACE_OPEN
namespace internal {
class AnyMetadata;
}  // namespace internal
PROTOBUF_NAMESPACE_CLOSE

// Internal implementation detail -- do not use these members.
struct TableStruct_tensorflow_2fcore_2fframework_2fconfig_2eproto {
  static const ::PROTOBUF_NAMESPACE_ID::internal::ParseTableField entries[]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::AuxiliaryParseTableField aux[]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::ParseTable schema[3]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::FieldMetadata field_metadata[];
  static const ::PROTOBUF_NAMESPACE_ID::internal::SerializationTable serialization_table[];
  static const ::PROTOBUF_NAMESPACE_ID::uint32 offsets[];
};
extern const ::PROTOBUF_NAMESPACE_ID::internal::DescriptorTable descriptor_table_tensorflow_2fcore_2fframework_2fconfig_2eproto;
namespace tensorflow {
class ConfigProto;
struct ConfigProtoDefaultTypeInternal;
extern ConfigProtoDefaultTypeInternal _ConfigProto_default_instance_;
class ConfigProto_DeviceCountEntry_DoNotUse;
struct ConfigProto_DeviceCountEntry_DoNotUseDefaultTypeInternal;
extern ConfigProto_DeviceCountEntry_DoNotUseDefaultTypeInternal _ConfigProto_DeviceCountEntry_DoNotUse_default_instance_;
class GPUOptions;
struct GPUOptionsDefaultTypeInternal;
extern GPUOptionsDefaultTypeInternal _GPUOptions_default_instance_;
}  // namespace tensorflow
PROTOBUF_NAMESPACE_OPEN
template<> ::tensorflow::ConfigProto* Arena::CreateMaybeMessage<::tensorflow::ConfigProto>(Arena*);
template<> ::tensorflow::ConfigProto_DeviceCountEntry_DoNotUse* Arena::CreateMaybeMessage<::tensorflow::ConfigProto_DeviceCountEntry_DoNotUse>(Arena*);
template<> ::tensorflow::GPUOptions* Arena::CreateMaybeMessage<::tensorflow::GPUOptions>(Arena*);
PROTOBUF_NAMESPACE_CLOSE
namespace tensorflow {

// ===================================================================

class GPUOptions PROTOBUF_FINAL :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:tensorflow.GPUOptions) */ {
 public:
  inline GPUOptions() : GPUOptions(nullptr) {}
  ~GPUOptions() override;
  explicit constexpr GPUOptions(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  GPUOptions(const GPUOptions& from);
  GPUOptions(GPUOptions&& from) noexcept
    : GPUOptions() {
    *this = ::std::move(from);
  }

  inline GPUOptions& operator=(const GPUOptions& from) {
    CopyFrom(from);
    return *this;
  }
  inline GPUOptions& operator=(GPUOptions&& from) noexcept {
    if (GetArena() == from.GetArena()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const GPUOptions& default_instance() {
    return *internal_default_instance();
  }
  static inline const GPUOptions* internal_default_instance() {
    return reinterpret_cast<const GPUOptions*>(
               &_GPUOptions_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    0;

  friend void swap(GPUOptions& a, GPUOptions& b) {
    a.Swap(&b);
  }
  inline void Swap(GPUOptions* other) {
    if (other == this) return;
    if (GetArena() == other->GetArena()) {
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(GPUOptions* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetArena() == other->GetArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  inline GPUOptions* New() const final {
    return CreateMaybeMessage<GPUOptions>(nullptr);
  }

  GPUOptions* New(::PROTOBUF_NAMESPACE_ID::Arena* arena) const final {
    return CreateMaybeMessage<GPUOptions>(arena);
  }
  void CopyFrom(const ::PROTOBUF_NAMESPACE_ID::Message& from) final;
  void MergeFrom(const ::PROTOBUF_NAMESPACE_ID::Message& from) final;
  void CopyFrom(const GPUOptions& from);
  void MergeFrom(const GPUOptions& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  ::PROTOBUF_NAMESPACE_ID::uint8* _InternalSerialize(
      ::PROTOBUF_NAMESPACE_ID::uint8* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  inline void SharedCtor();
  inline void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(GPUOptions* other);
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "tensorflow.GPUOptions";
  }
  protected:
  explicit GPUOptions(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kPerProcessGpuMemoryFractionFieldNumber = 1,
  };
  // double per_process_gpu_memory_fraction = 1;
  void clear_per_process_gpu_memory_fraction();
  double per_process_gpu_memory_fraction() const;
  void set_per_process_gpu_memory_fraction(double value);
  private:
  double _internal_per_process_gpu_memory_fraction() const;
  void _internal_set_per_process_gpu_memory_fraction(double value);
  public:

  // @@protoc_insertion_point(class_scope:tensorflow.GPUOptions)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  double per_process_gpu_memory_fraction_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_tensorflow_2fcore_2fframework_2fconfig_2eproto;
};
// -------------------------------------------------------------------

class ConfigProto_DeviceCountEntry_DoNotUse : public ::PROTOBUF_NAMESPACE_ID::internal::MapEntry<ConfigProto_DeviceCountEntry_DoNotUse, 
    std::string, ::PROTOBUF_NAMESPACE_ID::int32,
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_STRING,
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_INT32> {
public:
  typedef ::PROTOBUF_NAMESPACE_ID::internal::MapEntry<ConfigProto_DeviceCountEntry_DoNotUse, 
    std::string, ::PROTOBUF_NAMESPACE_ID::int32,
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_STRING,
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_INT32> SuperType;
  ConfigProto_DeviceCountEntry_DoNotUse();
  explicit constexpr ConfigProto_DeviceCountEntry_DoNotUse(
      ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);
  explicit ConfigProto_DeviceCountEntry_DoNotUse(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  void MergeFrom(const ConfigProto_DeviceCountEntry_DoNotUse& other);
  static const ConfigProto_DeviceCountEntry_DoNotUse* internal_default_instance() { return reinterpret_cast<const ConfigProto_DeviceCountEntry_DoNotUse*>(&_ConfigProto_DeviceCountEntry_DoNotUse_default_instance_); }
  static bool ValidateKey(std::string* s) {
    return ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(s->data(), static_cast<int>(s->size()), ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::PARSE, "tensorflow.ConfigProto.DeviceCountEntry.key");
 }
  static bool ValidateValue(void*) { return true; }
  void MergeFrom(const ::PROTOBUF_NAMESPACE_ID::Message& other) final;
  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;
};

// -------------------------------------------------------------------

class ConfigProto PROTOBUF_FINAL :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:tensorflow.ConfigProto) */ {
 public:
  inline ConfigProto() : ConfigProto(nullptr) {}
  ~ConfigProto() override;
  explicit constexpr ConfigProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ConfigProto(const ConfigProto& from);
  ConfigProto(ConfigProto&& from) noexcept
    : ConfigProto() {
    *this = ::std::move(from);
  }

  inline ConfigProto& operator=(const ConfigProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline ConfigProto& operator=(ConfigProto&& from) noexcept {
    if (GetArena() == from.GetArena()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const ConfigProto& default_instance() {
    return *internal_default_instance();
  }
  static inline const ConfigProto* internal_default_instance() {
    return reinterpret_cast<const ConfigProto*>(
               &_ConfigProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    2;

  friend void swap(ConfigProto& a, ConfigProto& b) {
    a.Swap(&b);
  }
  inline void Swap(ConfigProto* other) {
    if (other == this) return;
    if (GetArena() == other->GetArena()) {
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ConfigProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetArena() == other->GetArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  inline ConfigProto* New() const final {
    return CreateMaybeMessage<ConfigProto>(nullptr);
  }

  ConfigProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena) const final {
    return CreateMaybeMessage<ConfigProto>(arena);
  }
  void CopyFrom(const ::PROTOBUF_NAMESPACE_ID::Message& from) final;
  void MergeFrom(const ::PROTOBUF_NAMESPACE_ID::Message& from) final;
  void CopyFrom(const ConfigProto& from);
  void MergeFrom(const ConfigProto& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  ::PROTOBUF_NAMESPACE_ID::uint8* _InternalSerialize(
      ::PROTOBUF_NAMESPACE_ID::uint8* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  inline void SharedCtor();
  inline void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ConfigProto* other);
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "tensorflow.ConfigProto";
  }
  protected:
  explicit ConfigProto(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------


  // accessors -------------------------------------------------------

  enum : int {
    kDeviceCountFieldNumber = 1,
    kDeviceFiltersFieldNumber = 4,
    kGpuOptionsFieldNumber = 6,
    kIntraOpParallelismThreadsFieldNumber = 2,
    kPlacementPeriodFieldNumber = 3,
    kInterOpParallelismThreadsFieldNumber = 5,
    kAllowSoftPlacementFieldNumber = 7,
    kLogDevicePlacementFieldNumber = 8,
  };
  // map<string, int32> device_count = 1;
  int device_count_size() const;
  private:
  int _internal_device_count_size() const;
  public:
  void clear_device_count();
  private:
  const ::PROTOBUF_NAMESPACE_ID::Map< std::string, ::PROTOBUF_NAMESPACE_ID::int32 >&
      _internal_device_count() const;
  ::PROTOBUF_NAMESPACE_ID::Map< std::string, ::PROTOBUF_NAMESPACE_ID::int32 >*
      _internal_mutable_device_count();
  public:
  const ::PROTOBUF_NAMESPACE_ID::Map< std::string, ::PROTOBUF_NAMESPACE_ID::int32 >&
      device_count() const;
  ::PROTOBUF_NAMESPACE_ID::Map< std::string, ::PROTOBUF_NAMESPACE_ID::int32 >*
      mutable_device_count();

  // repeated string device_filters = 4;
  int device_filters_size() const;
  private:
  int _internal_device_filters_size() const;
  public:
  void clear_device_filters();
  const std::string& device_filters(int index) const;
  std::string* mutable_device_filters(int index);
  void set_device_filters(int index, const std::string& value);
  void set_device_filters(int index, std::string&& value);
  void set_device_filters(int index, const char* value);
  void set_device_filters(int index, const char* value, size_t size);
  std::string* add_device_filters();
  void add_device_filters(const std::string& value);
  void add_device_filters(std::string&& value);
  void add_device_filters(const char* value);
  void add_device_filters(const char* value, size_t size);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField<std::string>& device_filters() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField<std::string>* mutable_device_filters();
  private:
  const std::string& _internal_device_filters(int index) const;
  std::string* _internal_add_device_filters();
  public:

  // .tensorflow.GPUOptions gpu_options = 6;
  bool has_gpu_options() const;
  private:
  bool _internal_has_gpu_options() const;
  public:
  void clear_gpu_options();
  const ::tensorflow::GPUOptions& gpu_options() const;
  ::tensorflow::GPUOptions* release_gpu_options();
  ::tensorflow::GPUOptions* mutable_gpu_options();
  void set_allocated_gpu_options(::tensorflow::GPUOptions* gpu_options);
  private:
  const ::tensorflow::GPUOptions& _internal_gpu_options() const;
  ::tensorflow::GPUOptions* _internal_mutable_gpu_options();
  public:
  void unsafe_arena_set_allocated_gpu_options(
      ::tensorflow::GPUOptions* gpu_options);
  ::tensorflow::GPUOptions* unsafe_arena_release_gpu_options();

  // int32 intra_op_parallelism_threads = 2;
  void clear_intra_op_parallelism_threads();
  ::PROTOBUF_NAMESPACE_ID::int32 intra_op_parallelism_threads() const;
  void set_intra_op_parallelism_threads(::PROTOBUF_NAMESPACE_ID::int32 value);
  private:
  ::PROTOBUF_NAMESPACE_ID::int32 _internal_intra_op_parallelism_threads() const;
  void _internal_set_intra_op_parallelism_threads(::PROTOBUF_NAMESPACE_ID::int32 value);
  public:

  // int32 placement_period = 3;
  void clear_placement_period();
  ::PROTOBUF_NAMESPACE_ID::int32 placement_period() const;
  void set_placement_period(::PROTOBUF_NAMESPACE_ID::int32 value);
  private:
  ::PROTOBUF_NAMESPACE_ID::int32 _internal_placement_period() const;
  void _internal_set_placement_period(::PROTOBUF_NAMESPACE_ID::int32 value);
  public:

  // int32 inter_op_parallelism_threads = 5;
  void clear_inter_op_parallelism_threads();
  ::PROTOBUF_NAMESPACE_ID::int32 inter_op_parallelism_threads() const;
  void set_inter_op_parallelism_threads(::PROTOBUF_NAMESPACE_ID::int32 value);
  private:
  ::PROTOBUF_NAMESPACE_ID::int32 _internal_inter_op_parallelism_threads() const;
  void _internal_set_inter_op_parallelism_threads(::PROTOBUF_NAMESPACE_ID::int32 value);
  public:

  // bool allow_soft_placement = 7;
  void clear_allow_soft_placement();
  bool allow_soft_placement() const;
  void set_allow_soft_placement(bool value);
  private:
  bool _internal_allow_soft_placement() const;
  void _internal_set_allow_soft_placement(bool value);
  public:

  // bool log_device_placement = 8;
  void clear_log_device_placement();
  bool log_device_placement() const;
  void set_log_device_placement(bool value);
  private:
  bool _internal_log_device_placement() const;
  void _internal_set_log_device_placement(bool value);
  public:

  // @@protoc_insertion_point(class_scope:tensorflow.ConfigProto)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::internal::MapField<
      ConfigProto_DeviceCountEntry_DoNotUse,
      std::string, ::PROTOBUF_NAMESPACE_ID::int32,
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_STRING,
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_INT32> device_count_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField<std::string> device_filters_;
  ::tensorflow::GPUOptions* gpu_options_;
  ::PROTOBUF_NAMESPACE_ID::int32 intra_op_parallelism_threads_;
  ::PROTOBUF_NAMESPACE_ID::int32 placement_period_;
  ::PROTOBUF_NAMESPACE_ID::int32 inter_op_parallelism_threads_;
  bool allow_soft_placement_;
  bool log_device_placement_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_tensorflow_2fcore_2fframework_2fconfig_2eproto;
};
// ===================================================================


// ===================================================================

#ifdef __GNUC__
  #pragma GCC diagnostic push
  #pragma GCC diagnostic ignored "-Wstrict-aliasing"
#endif  // __GNUC__
// GPUOptions

// double per_process_gpu_memory_fraction = 1;
inline void GPUOptions::clear_per_process_gpu_memory_fraction() {
  per_process_gpu_memory_fraction_ = 0;
}
inline double GPUOptions::_internal_per_process_gpu_memory_fraction() const {
  return per_process_gpu_memory_fraction_;
}
inline double GPUOptions::per_process_gpu_memory_fraction() const {
  // @@protoc_insertion_point(field_get:tensorflow.GPUOptions.per_process_gpu_memory_fraction)
  return _internal_per_process_gpu_memory_fraction();
}
inline void GPUOptions::_internal_set_per_process_gpu_memory_fraction(double value) {
  
  per_process_gpu_memory_fraction_ = value;
}
inline void GPUOptions::set_per_process_gpu_memory_fraction(double value) {
  _internal_set_per_process_gpu_memory_fraction(value);
  // @@protoc_insertion_point(field_set:tensorflow.GPUOptions.per_process_gpu_memory_fraction)
}

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// ConfigProto

// map<string, int32> device_count = 1;
inline int ConfigProto::_internal_device_count_size() const {
  return device_count_.size();
}
inline int ConfigProto::device_count_size() const {
  return _internal_device_count_size();
}
inline void ConfigProto::clear_device_count() {
  device_count_.Clear();
}
inline const ::PROTOBUF_NAMESPACE_ID::Map< std::string, ::PROTOBUF_NAMESPACE_ID::int32 >&
ConfigProto::_internal_device_count() const {
  return device_count_.GetMap();
}
inline const ::PROTOBUF_NAMESPACE_ID::Map< std::string, ::PROTOBUF_NAMESPACE_ID::int32 >&
ConfigProto::device_count() const {
  // @@protoc_insertion_point(field_map:tensorflow.ConfigProto.device_count)
  return _internal_device_count();
}
inline ::PROTOBUF_NAMESPACE_ID::Map< std::string, ::PROTOBUF_NAMESPACE_ID::int32 >*
ConfigProto::_internal_mutable_device_count() {
  return device_count_.MutableMap();
}
inline ::PROTOBUF_NAMESPACE_ID::Map< std::string, ::PROTOBUF_NAMESPACE_ID::int32 >*
ConfigProto::mutable_device_count() {
  // @@protoc_insertion_point(field_mutable_map:tensorflow.ConfigProto.device_count)
  return _internal_mutable_device_count();
}

// int32 intra_op_parallelism_threads = 2;
inline void ConfigProto::clear_intra_op_parallelism_threads() {
  intra_op_parallelism_threads_ = 0;
}
inline ::PROTOBUF_NAMESPACE_ID::int32 ConfigProto::_internal_intra_op_parallelism_threads() const {
  return intra_op_parallelism_threads_;
}
inline ::PROTOBUF_NAMESPACE_ID::int32 ConfigProto::intra_op_parallelism_threads() const {
  // @@protoc_insertion_point(field_get:tensorflow.ConfigProto.intra_op_parallelism_threads)
  return _internal_intra_op_parallelism_threads();
}
inline void ConfigProto::_internal_set_intra_op_parallelism_threads(::PROTOBUF_NAMESPACE_ID::int32 value) {
  
  intra_op_parallelism_threads_ = value;
}
inline void ConfigProto::set_intra_op_parallelism_threads(::PROTOBUF_NAMESPACE_ID::int32 value) {
  _internal_set_intra_op_parallelism_threads(value);
  // @@protoc_insertion_point(field_set:tensorflow.ConfigProto.intra_op_parallelism_threads)
}

// int32 inter_op_parallelism_threads = 5;
inline void ConfigProto::clear_inter_op_parallelism_threads() {
  inter_op_parallelism_threads_ = 0;
}
inline ::PROTOBUF_NAMESPACE_ID::int32 ConfigProto::_internal_inter_op_parallelism_threads() const {
  return inter_op_parallelism_threads_;
}
inline ::PROTOBUF_NAMESPACE_ID::int32 ConfigProto::inter_op_parallelism_threads() const {
  // @@protoc_insertion_point(field_get:tensorflow.ConfigProto.inter_op_parallelism_threads)
  return _internal_inter_op_parallelism_threads();
}
inline void ConfigProto::_internal_set_inter_op_parallelism_threads(::PROTOBUF_NAMESPACE_ID::int32 value) {
  
  inter_op_parallelism_threads_ = value;
}
inline void ConfigProto::set_inter_op_parallelism_threads(::PROTOBUF_NAMESPACE_ID::int32 value) {
  _internal_set_inter_op_parallelism_threads(value);
  // @@protoc_insertion_point(field_set:tensorflow.ConfigProto.inter_op_parallelism_threads)
}

// int32 placement_period = 3;
inline void ConfigProto::clear_placement_period() {
  placement_period_ = 0;
}
inline ::PROTOBUF_NAMESPACE_ID::int32 ConfigProto::_internal_placement_period() const {
  return placement_period_;
}
inline ::PROTOBUF_NAMESPACE_ID::int32 ConfigProto::placement_period() const {
  // @@protoc_insertion_point(field_get:tensorflow.ConfigProto.placement_period)
  return _internal_placement_period();
}
inline void ConfigProto::_internal_set_placement_period(::PROTOBUF_NAMESPACE_ID::int32 value) {
  
  placement_period_ = value;
}
inline void ConfigProto::set_placement_period(::PROTOBUF_NAMESPACE_ID::int32 value) {
  _internal_set_placement_period(value);
  // @@protoc_insertion_point(field_set:tensorflow.ConfigProto.placement_period)
}

// repeated string device_filters = 4;
inline int ConfigProto::_internal_device_filters_size() const {
  return device_filters_.size();
}
inline int ConfigProto::device_filters_size() const {
  return _internal_device_filters_size();
}
inline void ConfigProto::clear_device_filters() {
  device_filters_.Clear();
}
inline std::string* ConfigProto::add_device_filters() {
  // @@protoc_insertion_point(field_add_mutable:tensorflow.ConfigProto.device_filters)
  return _internal_add_device_filters();
}
inline const std::string& ConfigProto::_internal_device_filters(int index) const {
  return device_filters_.Get(index);
}
inline const std::string& ConfigProto::device_filters(int index) const {
  // @@protoc_insertion_point(field_get:tensorflow.ConfigProto.device_filters)
  return _internal_device_filters(index);
}
inline std::string* ConfigProto::mutable_device_filters(int index) {
  // @@protoc_insertion_point(field_mutable:tensorflow.ConfigProto.device_filters)
  return device_filters_.Mutable(index);
}
inline void ConfigProto::set_device_filters(int index, const std::string& value) {
  // @@protoc_insertion_point(field_set:tensorflow.ConfigProto.device_filters)
  device_filters_.Mutable(index)->assign(value);
}
inline void ConfigProto::set_device_filters(int index, std::string&& value) {
  // @@protoc_insertion_point(field_set:tensorflow.ConfigProto.device_filters)
  device_filters_.Mutable(index)->assign(std::move(value));
}
inline void ConfigProto::set_device_filters(int index, const char* value) {
  GOOGLE_DCHECK(value != nullptr);
  device_filters_.Mutable(index)->assign(value);
  // @@protoc_insertion_point(field_set_char:tensorflow.ConfigProto.device_filters)
}
inline void ConfigProto::set_device_filters(int index, const char* value, size_t size) {
  device_filters_.Mutable(index)->assign(
    reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_set_pointer:tensorflow.ConfigProto.device_filters)
}
inline std::string* ConfigProto::_internal_add_device_filters() {
  return device_filters_.Add();
}
inline void ConfigProto::add_device_filters(const std::string& value) {
  device_filters_.Add()->assign(value);
  // @@protoc_insertion_point(field_add:tensorflow.ConfigProto.device_filters)
}
inline void ConfigProto::add_device_filters(std::string&& value) {
  device_filters_.Add(std::move(value));
  // @@protoc_insertion_point(field_add:tensorflow.ConfigProto.device_filters)
}
inline void ConfigProto::add_device_filters(const char* value) {
  GOOGLE_DCHECK(value != nullptr);
  device_filters_.Add()->assign(value);
  // @@protoc_insertion_point(field_add_char:tensorflow.ConfigProto.device_filters)
}
inline void ConfigProto::add_device_filters(const char* value, size_t size) {
  device_filters_.Add()->assign(reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_add_pointer:tensorflow.ConfigProto.device_filters)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField<std::string>&
ConfigProto::device_filters() const {
  // @@protoc_insertion_point(field_list:tensorflow.ConfigProto.device_filters)
  return device_filters_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField<std::string>*
ConfigProto::mutable_device_filters() {
  // @@protoc_insertion_point(field_mutable_list:tensorflow.ConfigProto.device_filters)
  return &device_filters_;
}

// .tensorflow.GPUOptions gpu_options = 6;
inline bool ConfigProto::_internal_has_gpu_options() const {
  return this != internal_default_instance() && gpu_options_ != nullptr;
}
inline bool ConfigProto::has_gpu_options() const {
  return _internal_has_gpu_options();
}
inline void ConfigProto::clear_gpu_options() {
  if (GetArena() == nullptr && gpu_options_ != nullptr) {
    delete gpu_options_;
  }
  gpu_options_ = nullptr;
}
inline const ::tensorflow::GPUOptions& ConfigProto::_internal_gpu_options() const {
  const ::tensorflow::GPUOptions* p = gpu_options_;
  return p != nullptr ? *p : reinterpret_cast<const ::tensorflow::GPUOptions&>(
      ::tensorflow::_GPUOptions_default_instance_);
}
inline const ::tensorflow::GPUOptions& ConfigProto::gpu_options() const {
  // @@protoc_insertion_point(field_get:tensorflow.ConfigProto.gpu_options)
  return _internal_gpu_options();
}
inline void ConfigProto::unsafe_arena_set_allocated_gpu_options(
    ::tensorflow::GPUOptions* gpu_options) {
  if (GetArena() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(gpu_options_);
  }
  gpu_options_ = gpu_options;
  if (gpu_options) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.ConfigProto.gpu_options)
}
inline ::tensorflow::GPUOptions* ConfigProto::release_gpu_options() {
  
  ::tensorflow::GPUOptions* temp = gpu_options_;
  gpu_options_ = nullptr;
  if (GetArena() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
  return temp;
}
inline ::tensorflow::GPUOptions* ConfigProto::unsafe_arena_release_gpu_options() {
  // @@protoc_insertion_point(field_release:tensorflow.ConfigProto.gpu_options)
  
  ::tensorflow::GPUOptions* temp = gpu_options_;
  gpu_options_ = nullptr;
  return temp;
}
inline ::tensorflow::GPUOptions* ConfigProto::_internal_mutable_gpu_options() {
  
  if (gpu_options_ == nullptr) {
    auto* p = CreateMaybeMessage<::tensorflow::GPUOptions>(GetArena());
    gpu_options_ = p;
  }
  return gpu_options_;
}
inline ::tensorflow::GPUOptions* ConfigProto::mutable_gpu_options() {
  // @@protoc_insertion_point(field_mutable:tensorflow.ConfigProto.gpu_options)
  return _internal_mutable_gpu_options();
}
inline void ConfigProto::set_allocated_gpu_options(::tensorflow::GPUOptions* gpu_options) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArena();
  if (message_arena == nullptr) {
    delete gpu_options_;
  }
  if (gpu_options) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
      ::PROTOBUF_NAMESPACE_ID::Arena::GetArena(gpu_options);
    if (message_arena != submessage_arena) {
      gpu_options = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, gpu_options, submessage_arena);
    }
    
  } else {
    
  }
  gpu_options_ = gpu_options;
  // @@protoc_insertion_point(field_set_allocated:tensorflow.ConfigProto.gpu_options)
}

// bool allow_soft_placement = 7;
inline void ConfigProto::clear_allow_soft_placement() {
  allow_soft_placement_ = false;
}
inline bool ConfigProto::_internal_allow_soft_placement() const {
  return allow_soft_placement_;
}
inline bool ConfigProto::allow_soft_placement() const {
  // @@protoc_insertion_point(field_get:tensorflow.ConfigProto.allow_soft_placement)
  return _internal_allow_soft_placement();
}
inline void ConfigProto::_internal_set_allow_soft_placement(bool value) {
  
  allow_soft_placement_ = value;
}
inline void ConfigProto::set_allow_soft_placement(bool value) {
  _internal_set_allow_soft_placement(value);
  // @@protoc_insertion_point(field_set:tensorflow.ConfigProto.allow_soft_placement)
}

// bool log_device_placement = 8;
inline void ConfigProto::clear_log_device_placement() {
  log_device_placement_ = false;
}
inline bool ConfigProto::_internal_log_device_placement() const {
  return log_device_placement_;
}
inline bool ConfigProto::log_device_placement() const {
  // @@protoc_insertion_point(field_get:tensorflow.ConfigProto.log_device_placement)
  return _internal_log_device_placement();
}
inline void ConfigProto::_internal_set_log_device_placement(bool value) {
  
  log_device_placement_ = value;
}
inline void ConfigProto::set_log_device_placement(bool value) {
  _internal_set_log_device_placement(value);
  // @@protoc_insertion_point(field_set:tensorflow.ConfigProto.log_device_placement)
}

#ifdef __GNUC__
  #pragma GCC diagnostic pop
#endif  // __GNUC__
// -------------------------------------------------------------------

// -------------------------------------------------------------------


// @@protoc_insertion_point(namespace_scope)

}  // namespace tensorflow

// @@protoc_insertion_point(global_scope)

#include <google/protobuf/port_undef.inc>
#endif  // GOOGLE_PROTOBUF_INCLUDED_GOOGLE_PROTOBUF_INCLUDED_tensorflow_2fcore_2fframework_2fconfig_2eproto
